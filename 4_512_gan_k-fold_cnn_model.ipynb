{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 16:30:09.845582: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-12 16:30:10.536675: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-12 16:30:12.388806: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/mmey/miniconda3/envs/wsl-tf/lib/\n",
      "2023-03-12 16:30:12.389151: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/mmey/miniconda3/envs/wsl-tf/lib/\n",
      "2023-03-12 16:30:12.389166: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import keras\n",
    "from keras.models import Sequential, save_model, load_model\n",
    "from keras.layers import Conv2D, Flatten, Dense, MaxPooling2D, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3297557231258211303\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 22655533056\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12380751551635393388\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 16:30:15.573094: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-12 16:30:16.080860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-12 16:30:16.179932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-12 16:30:16.180050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-12 16:30:20.599211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-12 16:30:20.599972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-12 16:30:20.600000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-03-12 16:30:20.600134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-12 16:30:20.600221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:0 with 21606 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 16:30:20.656784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-12 16:30:20.657036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-12 16:30:20.657119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import io\n",
    "import os\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are manually dividing as we do not want generated GAN MRI in the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "image_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pituitary', 'glioma', 'meningioma']\n"
     ]
    }
   ],
   "source": [
    "train_path = 'data/Training_GAN_512'\n",
    "labels = os.listdir(train_path)\n",
    "\n",
    "test_path = 'data/Test_GAN_512'\n",
    "labels = os.listdir(test_path)\n",
    "print(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all of the training and testing data and label them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training data and their labels.\n",
    "for i in labels:\n",
    "    folder_path = os.path.join(train_path, i)\n",
    "    for j in os.listdir(folder_path):\n",
    "        img = cv2.imread(os.path.join(folder_path, j))\n",
    "        img = cv2.resize(img, (image_size, image_size))\n",
    "        \n",
    "        X_train.append(img)\n",
    "        y_train.append(i)\n",
    "\n",
    "# Get the testing data and their labels.\n",
    "for i in labels:\n",
    "    folder_path = os.path.join(test_path, i)\n",
    "    for j in os.listdir(folder_path):\n",
    "        img = cv2.imread(os.path.join(folder_path, j))\n",
    "        img = cv2.resize(img, (image_size, image_size))\n",
    "        \n",
    "        X_test.append(img)\n",
    "        y_test.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2638, 128, 128, 3), (2638,), (1000, 128, 128, 3), (1000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn labels into number format for both training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_new = []\n",
    "y_test_new = []\n",
    "\n",
    "for i in y_train:\n",
    "    y_train_new.append(labels.index(i))\n",
    "y_new = np.array(y_train_new)\n",
    "\n",
    "for i in y_test:\n",
    "    y_test_new.append(labels.index(i))\n",
    "y_test_new = np.array(y_test_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## StratifiedKFold needs 1-dimensional Y array.\n",
    "# Do not convert to_categorical when using StratifiedKFold\n",
    "#y_new = tf.keras.utils.to_categorical(y_new)\n",
    "#y_new"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To get F1Score while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build CNN Model with K-Fold\n",
    "> We have X and y_new. We'll perform K-Fold cross validation<br>\n",
    "**iterative-stratification** is a project that provides scikit-learn compatible cross validators with stratification for multilabel data.<br>\n",
    "`!pip install iterative-stratification`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold, StratifiedKFold \n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_new = tf.keras.utils.to_categorical(y_train_new)\n",
    "y_test_new = tf.keras.utils.to_categorical(y_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 16:30:30.743175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-12 16:30:30.743345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-12 16:30:30.743394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-12 16:30:30.743722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-12 16:30:30.743740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-03-12 16:30:30.743838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-12 16:30:30.743868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21606 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-03-12 16:30:42.641755: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
      "2023-03-12 16:30:47.989429: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-03-12 16:30:48.236717: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-03-12 16:30:48.236781: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2023-03-12 16:30:48.474158: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-03-12 16:30:48.474338: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2023-03-12 16:30:50.372023: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 12ms/step - loss: 0.2504 - accuracy: 0.9087 - precision: 0.9213 - recall: 0.8897 - get_f1: 0.9067\n",
      "accuracy: 90.87%\n",
      "9/9 [==============================] - 1s 29ms/step - loss: 0.3091 - accuracy: 0.8902 - precision_1: 0.9027 - recall_1: 0.8788 - get_f1: 0.8879\n",
      "accuracy: 89.02%\n",
      "9/9 [==============================] - 1s 12ms/step - loss: 0.2827 - accuracy: 0.9091 - precision_2: 0.9331 - recall_2: 0.8977 - get_f1: 0.9112\n",
      "accuracy: 90.91%\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3331 - accuracy: 0.9015 - precision_3: 0.9203 - recall_3: 0.8750 - get_f1: 0.8662\n",
      "accuracy: 90.15%\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2849 - accuracy: 0.8864 - precision_4: 0.9084 - recall_4: 0.8636 - get_f1: 0.8835\n",
      "accuracy: 88.64%\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3577 - accuracy: 0.8826 - precision_5: 0.9080 - recall_5: 0.8598 - get_f1: 0.8398\n",
      "accuracy: 88.26%\n",
      "9/9 [==============================] - 1s 12ms/step - loss: 0.2804 - accuracy: 0.9011 - precision_6: 0.9268 - recall_6: 0.8669 - get_f1: 0.8570\n",
      "accuracy: 90.11%\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3487 - accuracy: 0.8788 - precision_7: 0.9113 - recall_7: 0.8561 - get_f1: 0.8550\n",
      "accuracy: 87.88%\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2711 - accuracy: 0.9167 - precision_8: 0.9291 - recall_8: 0.8939 - get_f1: 0.9136\n",
      "accuracy: 91.67%\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3604 - accuracy: 0.8636 - precision_9: 0.8760 - recall_9: 0.8561 - get_f1: 0.8262\n",
      "accuracy: 86.36%\n",
      "Accuracy (mean): 89.39% (+/- 1.56%)\n",
      "Precision (mean): 91.37% (+/- 1.58%)\n",
      "Recall (mean): 87.38% (+/- 1.49%)\n",
      "F1 (mean): 87.47% (+/- 2.91%)\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# define 10-fold cross validation test harness\n",
    "kfold = MultilabelStratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "precisionscores = []\n",
    "recallscores = []\n",
    "f1scores = []\n",
    "histories = []\n",
    "\n",
    "fold_no = 1\n",
    "best_fold = 1.5\n",
    "\n",
    "for train, test in kfold.split(X_train, y_train_new):\n",
    "\n",
    "    ## StratifiedKFold needs 1-dimensional Y array.\n",
    "    # Do not convert to_categorical when using StratifiedKFold\n",
    "    # instead, use to_categorical after splitting with k-fold...\n",
    "    #y_new = tf.keras.utils.to_categorical(y_new)\n",
    "    #y_new = tf.keras.utils.to_categorical(y_new)\n",
    "    #test\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), activation = 'relu', input_shape = (image_size, image_size, 3)))\n",
    "    model.add(Conv2D(64, (3,3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), activation = 'relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(128, (3,3), activation = 'relu'))\n",
    "    model.add(Conv2D(128, (3,3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(128, (3,3), activation = 'relu'))\n",
    "    model.add(Conv2D(256, (3,3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(256, activation = 'relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), get_f1])\n",
    "\n",
    "    # Fit the model\n",
    "    history = model.fit(X_train[train], y_train_new[train], epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    # Save the fold model\n",
    "    model_save_path = f'./fold_models/{fold_no}.h5'\n",
    "    save_model(model, model_save_path, save_format='h5')\n",
    "\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X_train[test], y_train_new[test], verbose=1)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    precisionscores.append(scores[2] * 100)\n",
    "    recallscores.append(scores[3] * 100)\n",
    "    f1scores.append(scores[4] * 100)\n",
    "    histories.append(history)\n",
    "\n",
    "    # take the fold history that has the best loss for our model \n",
    "    if (scores[0] < best_fold):\n",
    "        best_fold_history = history\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "print(\"Accuracy (mean): %.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "print(\"Precision (mean): %.2f%% (+/- %.2f%%)\" % (np.mean(precisionscores), np.std(precisionscores)))\n",
    "print(\"Recall (mean): %.2f%% (+/- %.2f%%)\" % (np.mean(recallscores), np.std(recallscores)))\n",
    "print(\"F1 (mean): %.2f%% (+/- %.2f%%)\" % (np.mean(f1scores), np.std(f1scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90.87452292442322, 89.01515007019043, 90.90909361839294, 90.15151262283325, 88.63636255264282, 88.25757503509521, 90.11406898498535, 87.87878751754761, 91.66666865348816, 86.36363744735718]\n"
     ]
    }
   ],
   "source": [
    "print(cvscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best performing model and train it\n",
    "> We'll train this model to be our final model to make predictions on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2638, 128, 128, 3), (2638, 3), (1000,), (1000, 3))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load selected model\n",
    "# FOLD 9\n",
    "loaded_model = load_model('./fold_models/8.h5', custom_objects={\"get_f1\": get_f1})\n",
    "#loaded_model.summary()\n",
    "\n",
    "X_train.shape, y_train_new.shape, y_test.shape, y_test_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2795 - accuracy: 0.8988 - precision_7: 0.9077 - recall_7: 0.8908 - get_f1: 0.8993\n",
      "Epoch 1: val_loss improved from inf to 0.90824, saving model to ./final_model.h5\n",
      "83/83 [==============================] - 5s 48ms/step - loss: 0.2795 - accuracy: 0.8988 - precision_7: 0.9077 - recall_7: 0.8908 - get_f1: 0.8993 - val_loss: 0.9082 - val_accuracy: 0.5210 - val_precision_7: 0.5287 - val_recall_7: 0.4980 - val_get_f1: 0.4992\n",
      "Epoch 2/15\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.2811 - accuracy: 0.8974 - precision_7: 0.9069 - recall_7: 0.8866 - get_f1: 0.8964\n",
      "Epoch 2: val_loss improved from 0.90824 to 0.74527, saving model to ./final_model.h5\n",
      "83/83 [==============================] - 3s 33ms/step - loss: 0.2821 - accuracy: 0.8958 - precision_7: 0.9054 - recall_7: 0.8851 - get_f1: 0.8942 - val_loss: 0.7453 - val_accuracy: 0.5630 - val_precision_7: 0.5932 - val_recall_7: 0.5220 - val_get_f1: 0.5232\n",
      "Epoch 3/15\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2536 - accuracy: 0.9083 - precision_7: 0.9145 - recall_7: 0.9007 - get_f1: 0.9067\n",
      "Epoch 3: val_loss did not improve from 0.74527\n",
      "83/83 [==============================] - 3s 35ms/step - loss: 0.2536 - accuracy: 0.9083 - precision_7: 0.9145 - recall_7: 0.9007 - get_f1: 0.9067 - val_loss: 0.9110 - val_accuracy: 0.4650 - val_precision_7: 0.4770 - val_recall_7: 0.4350 - val_get_f1: 0.4464\n",
      "Epoch 4/15\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2301 - accuracy: 0.9113 - precision_7: 0.9192 - recall_7: 0.9052 - get_f1: 0.9127\n",
      "Epoch 4: val_loss did not improve from 0.74527\n",
      "83/83 [==============================] - 3s 32ms/step - loss: 0.2301 - accuracy: 0.9113 - precision_7: 0.9192 - recall_7: 0.9052 - get_f1: 0.9127 - val_loss: 1.5064 - val_accuracy: 0.4570 - val_precision_7: 0.4568 - val_recall_7: 0.4440 - val_get_f1: 0.4399\n",
      "Epoch 5/15\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.2300 - accuracy: 0.9147 - precision_7: 0.9219 - recall_7: 0.9079 - get_f1: 0.9148\n",
      "Epoch 5: val_loss improved from 0.74527 to 0.72058, saving model to ./final_model.h5\n",
      "83/83 [==============================] - 3s 32ms/step - loss: 0.2300 - accuracy: 0.9147 - precision_7: 0.9219 - recall_7: 0.9079 - get_f1: 0.9148 - val_loss: 0.7206 - val_accuracy: 0.6450 - val_precision_7: 0.6592 - val_recall_7: 0.6190 - val_get_f1: 0.6155\n",
      "Epoch 6/15\n",
      "82/83 [============================>.] - ETA: 0s - loss: 0.2059 - accuracy: 0.9291 - precision_7: 0.9347 - recall_7: 0.9215 - get_f1: 0.9279\n",
      "Epoch 6: val_loss did not improve from 0.72058\n",
      "83/83 [==============================] - 3s 32ms/step - loss: 0.2051 - accuracy: 0.9295 - precision_7: 0.9350 - recall_7: 0.9219 - get_f1: 0.9288 - val_loss: 0.7819 - val_accuracy: 0.6210 - val_precision_7: 0.6275 - val_recall_7: 0.5880 - val_get_f1: 0.5881\n",
      "Epoch 7/15\n",
      "82/83 [============================>.] - ETA: 0s - loss: 0.1899 - accuracy: 0.9261 - precision_7: 0.9308 - recall_7: 0.9223 - get_f1: 0.9265\n",
      "Epoch 7: val_loss improved from 0.72058 to 0.68495, saving model to ./final_model.h5\n",
      "83/83 [==============================] - 3s 33ms/step - loss: 0.1904 - accuracy: 0.9257 - precision_7: 0.9304 - recall_7: 0.9219 - get_f1: 0.9256 - val_loss: 0.6850 - val_accuracy: 0.6350 - val_precision_7: 0.6462 - val_recall_7: 0.6010 - val_get_f1: 0.6020\n",
      "Epoch 8/15\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1905 - accuracy: 0.9291 - precision_7: 0.9352 - recall_7: 0.9249 - get_f1: 0.9303\n",
      "Epoch 8: val_loss did not improve from 0.68495\n",
      "83/83 [==============================] - 3s 32ms/step - loss: 0.1905 - accuracy: 0.9291 - precision_7: 0.9352 - recall_7: 0.9249 - get_f1: 0.9303 - val_loss: 0.9970 - val_accuracy: 0.6060 - val_precision_7: 0.6054 - val_recall_7: 0.5800 - val_get_f1: 0.5783\n",
      "Epoch 9/15\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1714 - accuracy: 0.9314 - precision_7: 0.9351 - recall_7: 0.9280 - get_f1: 0.9315\n",
      "Epoch 9: val_loss did not improve from 0.68495\n",
      "83/83 [==============================] - 3s 32ms/step - loss: 0.1714 - accuracy: 0.9314 - precision_7: 0.9351 - recall_7: 0.9280 - get_f1: 0.9315 - val_loss: 1.0000 - val_accuracy: 0.5200 - val_precision_7: 0.5232 - val_recall_7: 0.4970 - val_get_f1: 0.4963\n",
      "Epoch 10/15\n",
      "82/83 [============================>.] - ETA: 0s - loss: 0.1965 - accuracy: 0.9295 - precision_7: 0.9334 - recall_7: 0.9238 - get_f1: 0.9284\n",
      "Epoch 10: val_loss improved from 0.68495 to 0.61837, saving model to ./final_model.h5\n",
      "83/83 [==============================] - 3s 32ms/step - loss: 0.1975 - accuracy: 0.9291 - precision_7: 0.9330 - recall_7: 0.9234 - get_f1: 0.9276 - val_loss: 0.6184 - val_accuracy: 0.6920 - val_precision_7: 0.6936 - val_recall_7: 0.6610 - val_get_f1: 0.6596\n",
      "Epoch 11/15\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.1576 - accuracy: 0.9360 - precision_7: 0.9397 - recall_7: 0.9325 - get_f1: 0.9361\n",
      "Epoch 11: val_loss did not improve from 0.61837\n",
      "83/83 [==============================] - 3s 30ms/step - loss: 0.1569 - accuracy: 0.9363 - precision_7: 0.9400 - recall_7: 0.9325 - get_f1: 0.9357 - val_loss: 0.6693 - val_accuracy: 0.6670 - val_precision_7: 0.6801 - val_recall_7: 0.6420 - val_get_f1: 0.6473\n",
      "Epoch 12/15\n",
      "82/83 [============================>.] - ETA: 0s - loss: 0.1463 - accuracy: 0.9455 - precision_7: 0.9501 - recall_7: 0.9440 - get_f1: 0.9470\n",
      "Epoch 12: val_loss did not improve from 0.61837\n",
      "83/83 [==============================] - 3s 31ms/step - loss: 0.1461 - accuracy: 0.9454 - precision_7: 0.9500 - recall_7: 0.9439 - get_f1: 0.9468 - val_loss: 0.7039 - val_accuracy: 0.7170 - val_precision_7: 0.7291 - val_recall_7: 0.6970 - val_get_f1: 0.6969\n",
      "Epoch 13/15\n",
      "82/83 [============================>.] - ETA: 0s - loss: 0.1443 - accuracy: 0.9425 - precision_7: 0.9459 - recall_7: 0.9402 - get_f1: 0.9430\n",
      "Epoch 13: val_loss did not improve from 0.61837\n",
      "83/83 [==============================] - 3s 31ms/step - loss: 0.1435 - accuracy: 0.9428 - precision_7: 0.9462 - recall_7: 0.9405 - get_f1: 0.9437 - val_loss: 0.9826 - val_accuracy: 0.5730 - val_precision_7: 0.5780 - val_recall_7: 0.5520 - val_get_f1: 0.5476\n",
      "Epoch 14/15\n",
      "82/83 [============================>.] - ETA: 0s - loss: 0.1396 - accuracy: 0.9497 - precision_7: 0.9532 - recall_7: 0.9463 - get_f1: 0.9497\n",
      "Epoch 14: val_loss did not improve from 0.61837\n",
      "83/83 [==============================] - 3s 30ms/step - loss: 0.1434 - accuracy: 0.9488 - precision_7: 0.9523 - recall_7: 0.9454 - get_f1: 0.9477 - val_loss: 0.6271 - val_accuracy: 0.7470 - val_precision_7: 0.7536 - val_recall_7: 0.7340 - val_get_f1: 0.7279\n",
      "Epoch 15/15\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.1439 - accuracy: 0.9466 - precision_7: 0.9500 - recall_7: 0.9431 - get_f1: 0.9469\n",
      "Epoch 15: val_loss did not improve from 0.61837\n",
      "83/83 [==============================] - 3s 30ms/step - loss: 0.1439 - accuracy: 0.9466 - precision_7: 0.9500 - recall_7: 0.9431 - get_f1: 0.9469 - val_loss: 0.8109 - val_accuracy: 0.6260 - val_precision_7: 0.6463 - val_recall_7: 0.6030 - val_get_f1: 0.6018\n",
      "Epoch 15: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Define callbacks\n",
    "checkpoint_path = './final_model'\n",
    "os.mkdir(checkpoint_path)\n",
    "\n",
    "keras_callbacks = [\n",
    "ModelCheckpoint(checkpoint_path + '.h5', monitor='val_loss', save_best_only=True, mode='min', verbose=1),\n",
    "EarlyStopping(monitor='val_loss', mode='min', verbose= 1, patience= 5)\n",
    "]\n",
    "\n",
    "# fit final model.\n",
    "history = loaded_model.fit(X_train, y_train_new, validation_data=(X_test, y_test_new), epochs= 15, batch_size= 32, callbacks= keras_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1844, 128, 128, 3), (1844, 3))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_probs = model.predict(X_test, verbose=0)\n",
    "yhat_classes = np.argmax(yhat_probs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1844,), (1844, 3))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_classes.shape, y_test_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revert from label_encoder to 1-d pred. array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_new_2 = np.argmax(y_test_new, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1844,), (1844,))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_classes.shape, y_test_new_2.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Got predicitons. Now evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test_new_2, yhat_classes)\n",
    "precision = precision_score(y_test_new_2, yhat_classes, average='weighted')\n",
    "recall = recall_score(y_test_new_2, yhat_classes, average='weighted')\n",
    "f1 = f1_score(y_test_new_2, yhat_classes, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5954446854663774 | Precision: 0.7271697835352666 | Recall: 0.5954446854663774 | F1: 0.505723197761198\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {0} | Precision: {1} | Recall: {2} | F1: {3}'.format(acc, precision, recall, f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
